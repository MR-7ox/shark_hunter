{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e53a741-cec1-49d7-9ed9-05f54cc209c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 02:46:26,154 - INFO - Starting validation for 500 files...\n",
      "2025-10-05 02:46:28,058 - INFO - Validated 1 files...\n",
      "2025-10-05 02:48:08,680 - INFO - Validated 101 files...\n",
      "2025-10-05 02:49:41,282 - INFO - Validated 201 files...\n",
      "2025-10-05 02:51:09,664 - INFO - Validated 301 files...\n",
      "2025-10-05 02:52:27,454 - INFO - Validated 401 files...\n",
      "2025-10-05 02:53:46,309 - INFO - Validation took 440.15 seconds\n",
      "2025-10-05 02:53:46,312 - INFO - âœ… Found 500 valid files.\n",
      "2025-10-05 02:53:46,314 - INFO - \n",
      "1. Combining valid files using 'by_coords' (safe method)...\n",
      "2025-10-05 03:03:00,006 - ERROR - ðŸ›‘ FATAL ERROR during open_mfdataset: Could not find any dimension coordinates to use to order the datasets for concatenation\n",
      "2025-10-05 03:03:01,112 - ERROR - \n",
      "ðŸ›‘ Cannot proceed to ML training without the combined dataset.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from shapely.geometry import Polygon, Point\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up logging for better debugging and monitoring progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Configuration (Your specified path) ---\n",
    "folder_path = r\"D:\\Projects\\hackaton\\nasa\\final_2 pm data\\SWOT_SSH\"\n",
    "# ----------------------------------------\n",
    "\n",
    "# --- Polygon Check Function ---\n",
    "def is_point_in_polygon(lat, lon, polygon_coords):\n",
    "    \"\"\"Check if a given point (lat, lon) is inside the polygon.\"\"\"\n",
    "    polygon = Polygon(polygon_coords)\n",
    "    point = Point(lon, lat)\n",
    "    return polygon.contains(point)\n",
    "\n",
    "# --- Season Filter Function ---\n",
    "def filter_by_season(df, season):\n",
    "    \"\"\"Filter the DataFrame based on the season.\"\"\"\n",
    "    if season == 'summer':\n",
    "        return df[df['time'].dt.month.isin([6, 7, 8])]\n",
    "    elif season == 'winter':\n",
    "        return df[df['time'].dt.month.isin([12, 1, 2])]\n",
    "    else:\n",
    "        logging.warning(f\"Season {season} not recognized. Returning full dataset.\")\n",
    "        return df\n",
    "\n",
    "# --- Validation and File Processing ---\n",
    "def is_netcdf_file_valid(file_path):\n",
    "    \"\"\"Checks if a single NetCDF file is readable without errors.\"\"\"\n",
    "    try:\n",
    "        with xr.open_dataset(file_path, engine='netcdf4', decode_timedelta=False) as ds:\n",
    "            ds.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading file {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_valid_files(folder_path):\n",
    "    all_nc_files = glob.glob(os.path.join(folder_path, \"*.nc\"))\n",
    "    logging.info(f\"Starting validation for {len(all_nc_files)} files...\")\n",
    "\n",
    "    valid_nc_files = []\n",
    "    start_time = time.time()\n",
    "    for i, file in enumerate(all_nc_files):\n",
    "        if is_netcdf_file_valid(file):\n",
    "            valid_nc_files.append(file)\n",
    "        if i % 100 == 0:  # Print progress every 100 files\n",
    "            logging.info(f\"Validated {i+1} files...\")\n",
    "\n",
    "    logging.info(f\"Validation took {time.time() - start_time:.2f} seconds\")\n",
    "    logging.info(f\"âœ… Found {len(valid_nc_files)} valid files.\")\n",
    "    return valid_nc_files\n",
    "\n",
    "# --- STEP 1: Load Valid Files Safely ---\n",
    "# Run the validation to get the file list\n",
    "valid_nc_files = get_valid_files(folder_path) \n",
    "valid_nc_files.sort()  # Ensure chronological order\n",
    "\n",
    "logging.info(\"\\n1. Combining valid files using 'by_coords' (safe method)...\")\n",
    "\n",
    "# --- Safely open NetCDF files with error handling ---\n",
    "try:\n",
    "    # Use the more general and safer 'by_coords' method for merging time series data\n",
    "    shark_data_ds = xr.open_mfdataset(\n",
    "        valid_nc_files, \n",
    "        combine='by_coords',        # Recommended method for geospatial time series\n",
    "        chunks={'time': 10},         # Increase chunk size slightly to speed up computation\n",
    "        decode_timedelta=False,      # Suppress warnings\n",
    "        parallel=False               # Disable parallelism for simplicity\n",
    "    )\n",
    "    logging.info(\"âœ… Datasets successfully loaded into a virtual Dask array.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"ðŸ›‘ FATAL ERROR during open_mfdataset: {e}\")\n",
    "    shark_data_ds = None\n",
    "\n",
    "# Check if the loading was successful\n",
    "if shark_data_ds is None:\n",
    "    logging.error(\"\\nðŸ›‘ Cannot proceed to ML training without the combined dataset.\")\n",
    "else:\n",
    "    # ----------------------------------------------------------------------\n",
    "    # --- STEP 2: ML DATA PREPARATION (Using the same simulation structure) ---\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    logging.info(\"\\n2. Simulating shark observation data (REPLACE WITH YOUR REAL DATA)...\")\n",
    "    \n",
    "    # Simulate a small dataset of shark observations\n",
    "    num_obs = 1000\n",
    "    shark_observations_df = pd.DataFrame({\n",
    "        'time': pd.to_datetime('2023-01-01') + pd.to_timedelta(np.random.randint(0, 365, num_obs), unit='D'),\n",
    "        'lat': np.random.uniform(30, 35, num_obs),  # Target region from preview slice\n",
    "        'lon': np.random.uniform(-80, -75, num_obs),\n",
    "        'shark_present': np.random.randint(0, 2, num_obs) \n",
    "    })\n",
    "    \n",
    "    # --- Step 2.1: Filter by Polygon Input ---\n",
    "    polygon_coords = [(30, -80), (35, -80), (35, -75), (30, -75)]  # Example polygon (lat, lon)\n",
    "    logging.info(f\"Filtering data using polygon with coordinates: {polygon_coords}\")\n",
    "\n",
    "    shark_observations_df = shark_observations_df[\n",
    "        shark_observations_df.apply(lambda row: is_point_in_polygon(row['lat'], row['lon'], polygon_coords), axis=1)\n",
    "    ]\n",
    "    logging.info(f\"Filtered data to {len(shark_observations_df)} points inside the polygon.\")\n",
    "    \n",
    "    # --- Step 2.2: Filter by Season Input ---\n",
    "    season = 'summer'  # Example season input\n",
    "    shark_observations_df = filter_by_season(shark_observations_df, season)\n",
    "    logging.info(f\"Filtered data to {len(shark_observations_df)} points for season: {season}\")\n",
    "    \n",
    "    # 2.1 Extract Features by Merging\n",
    "    logging.info(\"3. Extracting features at observation points...\")\n",
    "    \n",
    "    # Round coordinates for nearest neighbor matching\n",
    "    obs_df = shark_observations_df.copy()\n",
    "    obs_df['lat_rounded'] = obs_df['lat'].round(2)\n",
    "    obs_df['lon_rounded'] = obs_df['lon'].round(2)\n",
    "\n",
    "    # Use xarray's nearest neighbor selection\n",
    "    merged_features_ds = shark_data_ds.sel(\n",
    "        time=obs_df.time, \n",
    "        lat=obs_df.lat_rounded, \n",
    "        lon=obs_df.lon_rounded, \n",
    "        method='nearest'\n",
    "    )\n",
    "    \n",
    "    # Convert the selected features to a DataFrame\n",
    "    training_df = merged_features_ds.to_dataframe().reset_index()\n",
    "    training_df = training_df.merge(\n",
    "        obs_df.drop(columns=['lat_rounded', 'lon_rounded']),\n",
    "        on=['time'], how='inner'\n",
    "    )\n",
    "    training_df = training_df.dropna(subset=['analyzed_sst'])  # Drop rows with missing 'analyzed_sst'\n",
    "\n",
    "    logging.info(\"4. Final Training Data Head:\")\n",
    "    logging.info(f\"{training_df.head()}\")\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # --- STEP 3: TRAIN XGBOOST CLASSIFIER ---\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    logging.info(\"\\n5. Training XGBoost Classifier...\")\n",
    "\n",
    "    # Feature Engineering\n",
    "    training_df['day_of_year'] = training_df['time'].dt.dayofyear\n",
    "    training_df['month'] = training_df['time'].dt.month\n",
    "\n",
    "    feature_cols = ['lat', 'lon', 'day_of_year', 'month', 'analyzed_sst', 'sea_ice_fraction']\n",
    "    target_col = 'shark_present'\n",
    "\n",
    "    # Check if 'sea_ice_fraction' column exists, if not, create it\n",
    "    if 'sea_ice_fraction' not in training_df.columns:\n",
    "        training_df['sea_ice_fraction'] = 0  # or np.nan, depending on how you'd handle missing data\n",
    "\n",
    "    X = training_df[feature_cols]\n",
    "    Y = training_df[target_col]\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    xgb_model = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # --- STEP 4: EVALUATE MODEL ---\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    Y_pred = xgb_model.predict(X_test)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sharkenv)",
   "language": "python",
   "name": "sharkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
